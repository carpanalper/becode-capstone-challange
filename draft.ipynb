{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.vrt.be/vrtnws/nl/net-binnen/\"\n",
    "response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_json = 'news.json'\n",
    "latest_news_json = 'latest_news.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(news_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(initial_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_json(new_data):\n",
    "    try:\n",
    "        # Read the existing data from the file if it exists\n",
    "        with open(news_json, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        data = []\n",
    "\n",
    "    # Create a set of existing links for quick lookup\n",
    "    existing_links = {entry['link'] for entry in data}\n",
    "\n",
    "    # Filter out the new data that already exists in the JSON file\n",
    "    unique_data = [entry for entry in new_data if entry['link'] not in existing_links]\n",
    "\n",
    "    # Append the unique data to the list\n",
    "    data.extend(unique_data)\n",
    "\n",
    "    # Write the updated data back to the JSON file\n",
    "    with open(news_json, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    with open(latest_news_json, 'w', encoding='utf-8') as file:\n",
    "        json.dump(unique_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <li> elements\n",
    "li_elements = soup.find('ul', class_='sc-ggpjZQ iJyTnM').find_all('li')\n",
    "scraped_news = []\n",
    "\n",
    "# Iterate over each <li> element\n",
    "for li in li_elements:\n",
    "    # Extract the <a> tag (link) and get the href attribute\n",
    "    link = li.find('a')\n",
    "    if link:\n",
    "        href = link.get('href')\n",
    "    else:\n",
    "        href = None\n",
    "       \n",
    "    # Find the <span> with the specified class\n",
    "    thema = li.find('span', class_='prose-captions text-text-mode line-clamp-1')\n",
    "    if thema:\n",
    "        topic = thema.text\n",
    "    else:\n",
    "        topic = None\n",
    "\n",
    "    # Find the <h3> element\n",
    "    header = li.find('h3')\n",
    "    if header:\n",
    "       title = header.text\n",
    "    else:\n",
    "        title = None\n",
    "\n",
    "    # Find the <time> element\n",
    "    time_tag = li.find('time') \n",
    "    if time_tag:\n",
    "        datetime_value = time_tag.get('datetime')\n",
    "    else:\n",
    "        datetime_value = None \n",
    "\n",
    "    #append data to the list\n",
    "    scraped_news.append({\n",
    "        'title': title,\n",
    "        'topic': topic,\n",
    "        'date' : datetime_value,\n",
    "        'link': href\n",
    "    })\n",
    "\n",
    "# Append the new data to the JSON file\n",
    "append_to_json(scraped_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 110\n"
     ]
    }
   ],
   "source": [
    "with open(news_json, 'r') as file:\n",
    "    total_data = json.load(file)\n",
    "    print(f\"Total entries: {len(total_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New entries: 9\n"
     ]
    }
   ],
   "source": [
    "with open(latest_news_json, 'r') as file:\n",
    "        new_data = json.load(file)\n",
    "        print(f\"New entries: {len(new_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
